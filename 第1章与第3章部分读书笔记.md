## <center>《强化学习》第一章与第三章（部分）读书笔记
##### <center> 智能科学与技术 2213530 张禹豪
注：引述原文内容为灰色方框中内容，其中包含其所处小节标题与页码；原文内容下方为读书笔记
### 第一章
#### 一、强化学习的特征以及与其他机器学习理论的不同
> **1.1 强化学习（p1）**
> &emsp;&emsp;学习者不会被告知应该采取什么动作，而是必须自己通过尝试去发现哪些动作会产生最丰厚的收益。在最有趣又最困难的案例中，动作往往影响的不仅仅是即时收益，也会影响下一个情境，从而影响随后的收益。这两个特征--试错和延迟收益是强化学习两个最重要最显著的特征。

&emsp;&emsp;强化学习的本质特征体现在两个方面：**试错探索**与**延迟收益**。知错探索意味着智能体不依赖外部直接指令，而是通过自主尝试不同动作，观察结果后逐步优化策略；延迟收益则意味着动作的影响具有时序性，当前动作不仅影响即时奖励，还会通过改变环境状态间接影响未来长期收益。这两个特征共同构成了强化学习区别于其他机器学习范式（如监督学习）的核心差异。
> **1.1 强化学习（p2）**
> &emsp;&emsp;我们使用了动态系统理论中的很多思想来形式化地定义强化学习问题，特别的例子就是针对“不完全可知的马尔可夫决策过程”的最优控制。......具备学习能力的智能体必须能够在某种程度上感知环境的状态，然后采取动作并影响环境状态。智能体必须同时拥有和环境状态相关的一个或多个明确的目标。马尔可夫决策过程就包含了这三个方面--感知、动作和目标，并将其以不失本质且简单的形式呈现。任何适用于解决这类问题的方法就是强化学习方法。

&emsp;&emsp;强化学习问题通过**马尔可夫决策过程**建模，其本质是动态系统理论在序列决策问题中的应用。智能体包含三大要素：
- 感知（Perception）：智能体需通过观测间接感知环境状态（尤其在部分可观测场景）；
- 动作（Action）：通过动作改变环境状态；
- 目标（Goal）：以最大化长期收益为优化目标。
> **1.1 强化学习（p2）**
> &emsp;&emsp;强化学习和现在机器学习领域中广泛使用的有监督学习不同，有监督学习是从外部监督者提供的带标注训练集中进行学习。每一个样本都是关于情境和标注的描述。......但是并不适用于从交互中学习这类问题。在交互问题中，我们不可能获得在所有情境下既正确又有代表性的动作示例。在一个未知领域，若想做到最好(收益最大)，智能体必须要能够从自身的经验中学习。

&emsp;&emsp;这段话核心对比了强化学习与有监督学习的本质差异。**有监督学习**依赖外部标注的静态数据集，通过输入（情境）-输出（标签）对直接学习映射关系，目标是**最小化预测误差**。而**强化学习**则通过智能体与环境的动态交互获取经验，无预定义标签，目标是**最大化长期累积收益**，需自主探索未知领域。
&emsp;&emsp;有监督学习适用于封闭静态问题（如分类、回归），但无法处理需动态交互、缺乏标注数据的开放序贯决策问题（如机器人控制、游戏博弈）。
> **1.1 强化学习（p3）**
> &emsp;&emsp;强化学习则从一个完整的、交互式的、目标导向的智能体出发，采取了相反的思路。所有强化学习的智能体都有一个明确的目标，即能够感知环境的各个方面，并可以选择动作来影响它们所处的环境。

&emsp;&emsp;这段话体现了强化学习智能体的设计哲学：强化学习从完整的智能体视角出发，要求智能体具备环境感知能力（获取状态信息）、动作决策能力（影响环境）和明确目标（最大化长期收益）。智能体与环境形成动态反馈循环，其动作不仅改变环境状态，同时通过环境反馈的奖励信号持续优化策略。
&emsp;&emsp;区别于有监督/无监督学习的“被动数据驱动”，强化学习是主动目标驱动，直接面向问题解决而非数据解释。
#### 二、强化学习要素
> **1.3 强化学习要素（p5）**
> &emsp;&emsp;策略定义了学习智能体在特定时间的行为方式。......策略本身是可以决定行为的，因此策略是强化学习智能体的核心。一般来说，策略可能是环境所在状态和智能体所采取的动作的随机函数。

&emsp;&emsp;**策略**是强化学习智能体的核心组件，其本质是行为决策规则：策略定义了智能体在特定环境状态下选择动作的规则，可以是确定性的或随机性的。策略直接决定智能体如何与环境交互，是连接感知（状态）与行动（动作）的桥梁，最终影响长期收益的累积。随机策略（如π(a∣s)）通过概率分布选择动作，支持探索未知状态，避免陷入局部最优。
> **1.3 强化学习要素（p6）**
> &emsp;&emsp;收益信号定义了强化学习问题中的目标。在每一步中，环境向强化学习智能体发送一个称为收益的标量数值。智能体的唯一目标是最大化长期总收益。也就是说，收益信号决定了，对于智能体来说何为好、何为坏。......因此，收益信号是改变策略的主要基础。......

&emsp;&emsp;**收益信号**是强化学习系统的核心驱动机制。收益是环境反馈给智能体的标量数值，直接量化动作的即时优劣，定义了智能体的终极优化目标（最大化长期总收益）。类似生物体的"愉悦/痛苦"机制，收益是智能体行为的即时反馈信号，驱动策略迭代方向。其具备以下核心特性：
- 时序性：当前收益可能影响未来状态序列的收益分布；
- 随机性：收益可能是状态和动作的随机函数（R(s,a)含概率性）；
- 策略修正依据：低收益触发策略调整，高收益强化当前策略。

> **1.3 强化学习要素（p6）**
> &emsp;&emsp;收益信号表明了在短时间内什么是好的，而价值函数则表示了从长远的角度看什么是好的。简单地说，一个状态的价值是一个智能体从这个状态开始，对将来累积的总收益的期望。尽管收益决定了环境状态直接、即时、内在的吸引力，但价值表示了接下来所有可能状态的长期期望。......然而，在制定和评估策略时，我们最关心的是价值。动作选择是基于对价值的判断做出的。.......

&emsp;&emsp;**收益**与**价值**分别代表短期反馈与长期判断。价值函数体现了从某一状态出发的长期期望累积收益，体现状态的战略意义。动作选择基于价值而非收益，因为价值是策略优劣的终极指标。
&emsp;&emsp;**动作价值函数**（Q-function）的决策意义：  
\[
Q^\pi(s,a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
\]  &emsp;&emsp;**最优策略**的数学表达：  
\[
\pi^*(s) = \arg\max_a Q^*(s,a)
\] &emsp;&emsp;最优策略始终选择最大化动作价值的动作，体现“价值驱动决策”原则。
> **1.3 强化学习要素（p6）**
> &emsp;&emsp;强化学习系统的第四个也是最后一个要素是对环境建立的模型。这是一种对环境的反应模式的模拟，或者更一般地说，它允许对外部环境的行为进行推断。......环境模型会被用于做规划。规划，就是在真正经历之前，先考虑未来可能发生的各种情境从而预先决定采取何种动作。使用环境模型和规划来解决强化学习问题的方法被称为有模型的方法。而简单的无模型的方法则是直接地试错，这与有目标地进行规划恰好相反。

&emsp;&emsp;**环境模型**是强化学习中有模型方法的核心。环境模型对环境的动态特性（状态转移概率、奖励分布）的数学或计算模拟，能够预测执行动作后的状态和奖励，它允许智能体在未实际执行动作时“想象”未来可能的状态序列，支持离线决策。
&emsp;&emsp;**规划**是基于环境模型的未来状态推演，通过模拟多步状态转移计算长期收益，以此指导策略优化。规划方法或者说有模型方法与无模型方法相对，前者依赖环境模型进行规划（如动态规划、AlphaGo的蒙特卡洛树搜索），后者直接基于环境交互经验进行策略迭代，无需环境模型（如Q-learning、策略梯度）。
#### 三、强化学习方法与进化方法的区别
> **1.4 局限性与适用范围（p7）**
> &emsp;&emsp;本书中讨论的大多数强化学习方法建立在对价值函数的估计上。但是这并不是解决强化学习问题的必由之路。举个例子，一些优化方法，如遗传算法、遗传规划、模拟退火算法以及其他一些方法，都可以用来解决强化学习问题，而不用显式地计算价值函数。这些方法采取大量静态策略，每个策略在扩展过的较长时间内与环境的一个独立实例进行交互。这些方法选择获取了最多收益的策略及其变种来产生下一代的策略，然后继续循环更新。我们称其为进化方法。...... 我们所关注的强化学习方法，是在与环境互动中学习的一类方法，而进化方法并不是。...... 尽管进化和学习之间有许多共性，并且两者常常相伴，但我们还是认为进化方法就其自身而言并不适用于强化学习问题，因此本书不会介绍与其相关的内容。

&emsp;&emsp;**强化学习方法**基于价值函数，通过估计状态或动作的长期价值V(s)或Q(s,a)指导策略优化，利用环境交互中的状态-动作序列信息，它的优势在于利用马尔可夫决策过程（MDP）的结构化信息（状态转移、奖励函数），学习效率高，适用于状态空间可感知、动作影响可建模的任务。
&emsp;&emsp;**进化方法**通过种群迭代（如遗传算法、模拟退火）直接优化策略，不显式计算价值函数，依赖策略的适应度（总收益）评估。其对状态感知错误或非MDP环境更鲁棒，适合黑箱优化，但是忽略了状态-动作关联性，搜索效率低，需大量策略评估。
#### 四、强化学习的早期历史发展
> **1.7 强化学习的早期历史（p13 - p21）**
> &emsp;&emsp;强化学习的历史发展有两条同样源远流长的主线，在交汇于现代强化学习之前它们是相互独立的。其中一条主线关注的是源于动物学习心理学的试错法。......另一条主线则关注最优控制的问题以及使用价值函数和动态规划的解决方案。......它们都与第三条不太明显的关注时序差分方法的主线有一定程度的关联。

&emsp;&emsp;**(1)试错法主线**（心理学与早期AI）

**1.心理学起源**（19世纪末-20世纪初）
- Thorndike的效应定律（1911）：提出行为选择通过奖励/惩罚强化的核心思想，奠定试错学习理论基础。
- 巴甫洛夫与次级强化（1927）：引入“强化”概念，扩展至刺激关联机制。
- 机械与计算实现（1930s-1970s）
- 早期机械装置（如Thomas Ross的迷宫机器、Shannon的Theseus）通过物理机制模拟试错学习。

**2.AI领域的探索：**
- 图灵（1948）提出“快乐-痛苦系统”雏形；
- Minsky（1954）构建SNARCs神经网络模拟强化学习；
- Michie的MENACE（1961）和GLEE（1968）系统在棋类与控制问题中实现试错学习。

**3.理论深化与算法突破**（1970s-1980s）
- Klopf的享乐主义假说（1970s）：强调行为目标导向性，推动强化学习与监督学习的区分。
- 行动器-评判器架构（Barto & Sutton，1983）：结合试错与预测机制，应用于倒立摆控制问题。

&emsp;&emsp;**(2)最优控制主线**（动态规划与系统工程）

**1.理论基础构建**（1950s-1960s）
- Bellman的贡献：
提出动态规划（1957）与贝尔曼方程，解决状态空间优化问题；
定义马尔可夫决策过程（MDP），奠定形式化框架。
- Howard的策略迭代法（1960）：完善MDP求解方法。

**2.动态规划扩展**（1970s-1980s）
- 处理部分可观测环境（POMDP）、异步计算等扩展，但因“维度灾难”受限。
- 与学习的初步结合：
Witten（1977）首次将动态规划与在线学习结合；
Werbos（1987）提出动态规划与神经网络的关联。

&emsp;&emsp;**(3)时序差分（TD）主线**（预测与价值函数）

**1.早期萌芽**（1950s-1970s）
- Samuel的跳棋程序（1959）：首次实现基于未来状态预测的估值函数在线更新，隐含TD思想。
- 心理学启发：次级强化物概念推动连续预测机制的发展。

**2.算法形式化**（1980s）
- Sutton的TD理论（1988）：提出TD(λ)算法，将其抽象为通用预测方法，证明收敛性。
- Q学习的突破（Watkins，1989）：将TD学习与最优控制结合，实现无模型MDP求解。

&emsp;&emsp;**(4)三条主线的交汇与强化学习成型**
- 1980年代末整合：
1.试错法与TD结合：行动器-评判器架构将行为选择与价值预测统一。
2.动态规划的在线化：Q学习等算法通过TD误差更新价值函数，解决模型依赖问题。
3.应用验证：Tesauro的TD-Gammon（1992）在西洋双陆棋中展现强大性能，引发广泛关注。
- 跨学科影响：
1.神经科学发现多巴胺信号与TD误差的匹配（1990s），揭示生物学机制与算法的深层关联。
2.经济学与博弈论引入强化学习，模拟非理性决策行为。

### 第三章
#### 一、有限马尔科夫决策过程相关概念
> **3.1 "智能体-环境”交互接口(p45,p46,p47)**
> &emsp;&emsp;MDP 就是一种通过交互式学习来实现目标的理论框架。进行学习及实施决策的机器被称为智能体(agent)。智能体之外所有与其相互作用的事物都被称为环境。...... 
> &emsp;&emsp;...... 环境也会产生一个收益，通常是特定的数值，这就是智能体在动作选择过程中想要最大化的的目标。...... 随机变量$R_t$和$S_t$具有定义明确的离散概率分布，并且只依赖于前继状态和动作。...... 也就是说，$S_t$和$R_t$的每个可能的值出现的概率只取决于前一个状态$S_{t-1}$和前一个动作$A_{t-1}。$

&emsp;&emsp;**马尔可夫决策过程**是强化学习的基础数学框架，描述智能体与环境通过**状态（State）**、**动作（Action）**、**收益（Reward）** 三者交互的序列决策问题。 
&emsp;&emsp;**马尔可夫性质（Markov Property）**：下一时刻状态$S_t$和收益$R_t$的分布**仅依赖前一时刻状态$S_{t-1}$和动作$A_{t-1}$**，与更早的历史无关。  
 $$P(S_t, R_t | S_{t-1}, A_{t-1}, S_{t-2}, A_{t-2}, \dots) = P(S_t, R_t | S_{t-1}, A_{t-1})$$ &emsp;&emsp;这一性质的意义在于将复杂的历史交互简化为当前状态与动作的函数，极大降低建模与计算复杂度。  


> **3.1 "智能体-环境”交互接口(p48)**
> &emsp;&emsp;我们遵循的一般规则是，智能体不能改变的事物都被认为是在外部的，即是环境的-部分。但我们并不是假定智能体对环境一无所知。...... 智能体和环境的界限划分仅仅决定了智能体进行绝对控制的边界，而并不是其知识的边界。...... 
> &emsp;&emsp;MDP 框架是目标导向的交互式学习问题的一个高度抽象。它提出，无论感官、记忆和控制设备细节如何，无论要实现何种目标，任何目标导向的行为的学习问题都可以概括为智能体及其环境之间来回传递的三个信号:一个信号用来表示智能体做出的选择(行动)，一个信号用来表示做出该选择的基础(状态)，还有一个信号用来定义智能体的目标(收益)。

**智能体与环境的界限划分** 
&emsp;&emsp;智能体无法直接改变的任何事物均属于环境（如物理法则、其他智能体行为）。  
- 界限划分的本质是**控制权的边界**，而非知识的边界。智能体仍可通过学习或先验知识理解环境动态。  
- **示例**：自动驾驶汽车中，车辆的传感器与控制器属于智能体，道路条件和其他车辆属于环境；但智能体可通过预测模型预判环境变化（如其他车辆的轨迹）。 

**MDP框架的普适性**： 
- 无论具体任务（游戏、机器人控制、投资决策），均可抽象为此三信号交互模式。  
- 剥离硬件细节（如摄像头类型）与目标类型（如经济收益或生存时长），聚焦决策逻辑的数学本质。


> **3.2 目标和收益(p51)**
> &emsp;&emsp;...... 这意味着需要最大化的不是当前收益，而是长期的累积收益。我们可以将这种非正式想法清楚地表述为收益假设:我们所有的“目标”或“目的”都可以归结为:最大化智能体接收到的标量信号(称之为收益)累积和的概率期望值。使用收益信号来形式化目标是强化学习最显著的特征之一。

**收益假设（Reward Hypothesis）**：所有目标均可归结为最大化智能体接收的**标量收益信号**的**长期累积期望值**，即：  
     $$\text{目标} = \max \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right]$$ 其中`γ ∈ [0,1]`为折扣因子，平衡即时收益与未来收益的重要性。  
   - **与短期目标的区别**：  
     - 避免短视行为（如“涸泽而渔”），强调通过当前动作影响未来状态的长期价值。  
     - 收益信号需隐含时间维度上的因果关系（如吸烟的即时愉悦与长期健康惩罚的权衡）。  


> **3.2 目标和收益(p51)**
> &emsp;&emsp;...... 智能体总是学习如何最大化收益。如果我们想要它为我们做某件事，我们提供收益的方式必须要使得智能体在最大化收益的同时也实现我们的目标。因此，至关重要的一点就是，我们设立收益的方式要能真正表明我们的目标。特别地，收益信号并不是传授智能体如何实现目标的先验知识。...收益信号只能用来传达什么是你想要实现的目标，而不是如何实现这个目标。

&emsp;&emsp;收益信号仅定义智能体需实现的**最终目标**（如"到达终点"），而非指定实现路径（如"先左转再加速"）。  
&emsp;&emsp;收益不包含实现目标的先验知识（如不规定具体动作序列）。  
&emsp;&emsp;智能体必须通过试错自主发现最大化收益的策略（如通过Q-learning发现最优路径）。  


> **3.3 回报和分幕(p52)**
> &emsp;&emsp;我们知道智能体的目标就是最大限度地提高长期收益。...... 一般来说，我们寻求的是最大化期望回报，记为$G_t$。
> &emsp;&emsp; ......在这类应用中，智能体和环境的交互能被自然地分成一系列子序列(每个序列都存在最终时刻).我们称每个子序列为幕,......每幕都以一种特殊状态结束，称之为终结状态。...... 具有这种分幕重复特性的任务称为分幕式任务。
> &emsp;&emsp;另一方面，在许多情况下，智能体-环境交互不一定能被自然地分为单独的幕，而是持续不断地发生。例如，我们很自然地就会想到一个连续的过程控制任务或者长期运行机器人的应用。我们称这些为持续性任务。
> &emsp;&emsp;我们需要引人一个额外概念，即折扣。根据这种方法，智能体尝试选择动作，使得它在未来收到的经过折扣系数加权后的(我们成为“折后”)收益总和是最大化的。
> &emsp;&emsp;折扣率决定了未来收益的现值:未来时刻的收益值只有它的当前值的$γ^{k-1}$倍。如果γ<1,那么只要收益序列{$R_k$}有界，式(3.8)中的无限序列总和就是一个有限值。如果γ=0，那么智能体是“目光短浅的”，即只关心最大化当前收益。

   - **分幕式任务（Episodic Tasks）**  
     - 定义：智能体与环境的交互被划分为多个独立子序列（幕），每幕在**终结状态**结束（如棋局终盘、游戏通关）。  
     - 数学特征：  
       $$G_t = R_{t+1} + R_{t+2} + \dots + R_T \quad (T为终止时刻)$$  
    
- **折扣机制（Discounting）**  
    - **物理意义**：  
         - 未来收益的现值 = 实际收益 × $γ^{k-1}$（k为未来步数），反映"时间价值"（如当前100元比1年后100元更值钱）。  
         - γ趋近1时智能体更"远见"，γ趋近0时更"短视"。  
    - **数学必要性**：当γ<1时确保无限累积收益收敛（如γ=0.9时，$\sum_{k=0}^{\infty} 0.9^k = 10$）。 

- **分幕式与持续性任务的统一表达**  
   - 分幕式任务可视为γ=1且T有限的特殊情况：  
     $$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$  
   - 当γ=1且T→∞时，需假设收益序列绝对可和（实际场景中通常需折扣）。 
- **折扣因子对策略的影响**  
   - **短视策略（γ=0）**：  
     $$G_t = R_{t+1}$$  仅优化即时收益，易陷入局部最优（如掠夺性开采资源）。  
   - **长远策略（γ→1）**：  
     $$G_t \approx \sum_{k=0}^{\infty} R_{t+k+1}$$  需平衡当前动作对未来状态的长期影响（如环保投资）。  