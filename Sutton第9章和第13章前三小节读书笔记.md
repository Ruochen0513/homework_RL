# Sutton第9章和第13章前三小节读书笔记
### 张禹豪 2213530
### 第9章 基于函数逼近的同轨策略预测
## 一、章节概述
本章聚焦强化学习中的函数逼近方法，重点研究如何利用同轨策略（on-policy）数据估计状态价值函数 ($v_{\pi}$ )。与表格型方法不同，函数逼近通过参数化函数 ( $\hat{v}(s,\mathbf{w})$ )（$( \mathbf{w} \in \mathbb{R}^d$ ) 为权值向量）近似  $v_\pi(s)$，适用于状态空间极大或连续的场景。核心内容包括价值函数逼近的定义、预测目标（均方价值误差VE）、随机梯度与半梯度方法，以及线性函数逼近的特殊情形。
## 二、核心概念解析
### 2.1 价值函数逼近
- **基本思想**：将强化学习的更新（如蒙特卡洛的 ( $S_t \rightarrow G_t$ )、TD(0)的 ( $S_t \rightarrow R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}))$视为有监督学习的“输入-输出”样本，通过函数逼近器（如神经网络、决策树、线性模型）拟合这些样本，实现对 $v_{\pi}$ 的泛化估计。
- **适用条件**：需处理在线学习（逐步数据）和非平稳目标（如GPI中策略变化或自举法导致的目标变化）。
### 2.2 预测目标：均方价值误差（VE）
- **定义**：衡量近似价值函数与真实价值函数的差异，公式为：
  $$
  	\overline{VE}(\mathbf{w}) = \sum_s \mu(s) \left( v_\pi(s) - \hat{v}(s, \mathbf{w}) 
\right)^2
  $$
  其中 ( $\mu(s)$ ) 是状态分布（同轨策略下为状态访问频率的归一化），反映对不同状态误差的重视程度。
- **目标**：寻找权值 ( $\mathbf{w}^*$ ) 最小化 ( $\overline{VE}(\mathbf{w})$ )，但复杂模型（如神经网络）通常仅能找到局部最优。
### 2.3 随机梯度（SGD）与半梯度方法
- **随机梯度下降（SGD）**：基于单样本的梯度更新，权值调整方向为样本误差的负梯度：
  $$
  \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}_t) 
\right] 
\nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w}_t)
  $$
  其中 ( $U_t$ ) 是目标值（如蒙特卡洛的 ( $G_t$ )）。若 ( $U_t$ ) 无偏（( $\mathbb{E}[U_t | S_t] = v_\pi(S_t) )$），SGD可收敛到局部最优。
- **半梯度方法**：当 ( $U_t$ ) 依赖当前权值（如TD(0)的 ( $R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$)），仅计算部分梯度（忽略 ( $U_t$ ) 对 ( $\mathbf{w}$ ) 的依赖），虽无严格收敛保证，但学习速度快且支持在线更新。
### 2.4 线性函数逼近
- **形式**：( $\hat{v}(s, \mathbf{w}) =\mathbf{w}^	\top \mathbf{x}(s)$ )，其中 ( $\mathbf{x}(s)$ ) 是状态 ( s ) 的特征向量（基函数）。
- **优势**：梯度计算简单（( $\nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) = \mathbf{x}(s)$)），且线性模型的VE存在全局最优解。
## 三、关键算法
### 3.1 梯度蒙特卡洛算法
- **输入**：策略 ( $\pi$ )、可微函数 ( $\hat{v}$ )、步长 ( $\alpha$ )。
- **流程**：
  1. 按 ( $\pi$ ) 生成一幕数据 ( $S_0$, $A_0$, $R_1$, $\dots$, $S_T$ )；
  2. 对每一步 ( t )，更新 ( $\mathbf{w} \leftarrow \mathbf{w} + \alpha [G_t - \hat{v}(\S_t, \mathbf{w})] \nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w})$ )。
### 3.2 半梯度TD(0)算法
- **输入**：策略 ( $\pi$ )、可微函数 ( $\hat{v}$ )（终止状态值为0）、步长 ( $\alpha$ )。
- **流程**：
  1. 初始化状态 ( S )；
  2. 循环：选择 ( $A \sim \pi(\cdot|S)$ )，执行后观察 ( R, S' )；
  3. 更新 ( $\mathbf{w} \leftarrow \mathbf{w} + \alpha [R + \gamma \hat{v}(S', \mathbf{w}) - \hat{v}(S, \mathbf{w})] \nabla_{\mathbf{w}} \hat{v}(S, \mathbf{w})$ )；
  4. 直到 ( S' ) 为终止状态。
## 四、示例与总结
- **示例**：1000状态随机游走任务中，状态聚合（分组估计）的梯度蒙特卡洛算法通过10组划分（每组100状态），得到阶梯状近似价值函数，其误差分布受状态访问频率 ( $\mu(s)$ ) 影响显著（如边缘组估计偏向高频子状态）。
- **总结**：函数逼近扩展了强化学习的适用场景（大/连续状态空间），但需平衡泛化与误差控制；SGD与半梯度方法是核心工具，线性模型因计算简单和全局最优性成为基础研究重点。
## 五、线性方法的特征构造扩展
### 5.1 多项式基
- **适用场景**：状态由数字分量表示（如杆平衡的位置/速度、杰克租车的车辆数），与插值/回归任务类似。
- **特征形式**：以二维状态为例，基础特征为$(1, s_1, s_2, s_1s_2)$（含常数项和交互项），高阶多项式可包含$s_1^2, s_2^2, s_1^2s_2$等，支持复杂交互的近似。
- **扩展到k维**：n阶多项式基特征数为$(n+1)^k$（指数增长），高维场景需结合先验知识或自动选择方法筛选特征。
- **练习思考**：式(9.17)中k维状态的n阶多项式基特征数为$(n+1)^k$（每个维度取0~n次幂的组合）；示例特征向量对应n=2（含二次项），$C_{i,j}$为各维度的幂次（如$s_1^2$对应$C_{i,1}=2$，$C_{i,2}=0$）。
### 5.2 傅立叶基
- **核心思想**：利用正弦/余弦函数的周期性组合近似任意函数，适用于有界区间的非周期函数（通过设置周期T为区间长度）。
- **一维形式**：n阶傅立叶余弦基特征为$cos(i\pi s)$（$i=0,1,...,n$），覆盖区间$[0,1]$，偶函数特性（仅余弦）更易近似“半偶”函数。
- **多维扩展**：k维n阶特征为$\cos(\pi s^T c)$（$c$为各维度取0~n的整数向量），特征数$(n+1)^k$。向量$c$的零分量表示该维度为常数，非零分量表示频率（如$c=(1,0)$表示第一维频率1，第二维常数）。
- **步长建议**：对特征$x_i$（对应向量$c$），步长$\alpha_i = \alpha / \sqrt{c_1^2 + ... + c_k^2}$（$c$全零时$\alpha_i = \alpha$）。
- **优缺点**：优于多项式基（如1000状态随机游走任务），但对不连续函数需高频基函数避免“波动”；全局特征（非零覆盖）难表征局部特性。
### 5.3 粗编码
- **定义**：使用重叠的感受野（如圆形区域）构造二值特征（状态在感受野内则特征为1），通过特征重叠实现泛化。
- **泛化特性**：感受野大小决定初始泛化范围（宽野→强泛化，窄野→局部调整），但最终精度由特征数量主导（足够多特征可实现精细判别）。
- **示例验证**：一维方波函数学习中，宽/中/窄感受野的早期学习曲线差异大（宽野更快平滑），但渐近误差相近（特征数量主导）。
### 5.4 瓦片编码
- **核心形式**：多维连续空间的粗编码，通过多个覆盖（均匀网格划分）的瓦片（正方形感受野）构造特征。单一覆盖退化为状态聚合（仅同一瓦片内泛化），多覆盖通过重叠瓦片实现更灵活的泛化。
- **优势**：计算高效（二进制特征操作简单），适用于现代数字计算机，是实践中最常用的连续状态特征表达方法。
### 5.5 径向基函数（RBF）
径向基函数（radial basis function, RBF）是粗编码在连续特征中的自然推广。每个特征取值为[0,1]区间内的实数值，反映特征的“激活程度”。典型RBF采用高斯响应函数：
$$ x_i(s) = exp\left(-\frac{||s - c_i||^2}{2\sigma_i^2}\right) $$
其中 ( $c_i$ ) 为特征中心，( $\sigma_i$ ) 为宽度，范数可根据任务选择（如欧几里得距离）。
RBF的优势在于生成光滑可微的近似函数，但计算复杂度高于瓦片编码，高维场景下边界激活优化困难。RBF网络作为线性函数逼近器，若调整中心和宽度则变为非线性逼近器，虽可能更精确，但需更多调参且计算成本高。
## 六、步长参数的手动选择
大部分SGD方法需手动设置步长 ( $\alpha$ )。理论上随机近似要求步长缓慢递减（如 ( $\alpha_t=1/t$ )），但实际中：
- 表格型方法：($\alpha=1/T$ )（T为状态经验次数），使估计在T次经验后收敛；
- 线性函数逼近：经验法则 ( $\alpha=(T\mathbb{E}[x^\top x])^{-1}$)（x为特征向量），适用于特征长度稳定场景。
## 七、非线性函数逼近：人工神经网络
人工神经网络（ANN）通过多层非线性单元实现复杂函数逼近，核心特性包括：
### 7.1 网络结构与计算
前向ANN无环结构，包含输入层、隐层和输出层。计算单元采用半线性模型：先计算输入加权和，再通过激活函数（如sigmoid、ReLU）输出。无隐层ANN仅能表示简单映射，单隐层（足够sigmoid单元）可在紧集上任意逼近连续函数（Cybenko, 1989）。
### 7.2 训练与挑战
ANN通常用随机梯度下降训练，梯度计算通过反向传播实现。深度ANN（多隐层）虽能学习层次化特征，但面临：
- 过拟合：参数量大时泛化困难，可通过早停、正则化、随机丢弃（Dropout）缓解；
- 梯度消失/爆炸：反向传播时梯度衰减或激增，可通过批量归一化（BatchNorm）、残差学习（ResNet）优化。
### 7.3 深度网络优化技术
- 预训练（Hinton et al., 2006）：逐层无监督训练提取输入统计特征，作为有监督训练的初始权值；
- 批量归一化（Ioffe & Szegedy, 2015）：层间输出归一化，加速深度网络训练；
- 残差学习（He et al., 2016）：通过“捷径连接”学习残差函数，缓解深层网络性能退化。
这些技术推动了深度ANN在强化学习中的应用。

---

### 第13章 策略梯度方法
## 一、章节概述
本章聚焦直接学习参数化策略的强化学习方法（策略梯度方法），与传统基于价值函数的方法不同，动作选择不直接依赖价值函数。核心内容包括策略参数化的优势、策略梯度定理的理论支持，以及蒙特卡洛策略梯度算法（REINFORCE）等。
## 二、核心概念解析
### 2.1 策略梯度方法定义
策略梯度方法通过优化性能指标J(θ)的梯度（$∇J(θ)$）来更新策略参数$θ$，目标是最大化性能指标。更新规则为：$θ_{t+1} = θ_t + α∇J(θ_t)$，其中$∇J(θ_t)$是梯度的随机估计。同时学习策略和价值函数的方法称为行动器-评判器方法（行动器：策略；评判器：价值函数）。
### 2.2 策略参数化及其优势
- **参数化形式**：对于离散动作空间，常用指数柔性最大化分布（softmax）参数化策略，即$π(a|s,θ) = e^{h(s,a,θ)} / Σ_b e^{h(s,b,θ)}$，其中$h(s,a,θ)$为动作偏好值（可线性组合或神经网络表示）。
- **优势**：
  - 可趋向确定策略（偏好值差异增大时，最优动作概率趋近1）；
  - 支持随机最优策略（如非完全信息游戏中的混合策略）；
  - 策略函数可能比价值函数更简单（依问题而异，学习更快）；
  - 便于引入先验知识（通过设计偏好函数的结构）。
### 2.3 策略梯度定理
- **核心意义**：提供了性能指标梯度$∇J(θ)$的解析表达式，避免了对状态分布求导的复杂性。
- **分幕式情况表达式**：$∇J(θ) ∝ Σ_s μ(s) Σ_a [∇π(a|s,θ)/π(a|s,θ)] q_π(s,a)$，其中$μ(s)$是同轨策略状态分布，$q_π(s,a)$是动作价值函数。
- **作用**：为梯度上升更新提供了理论依据，确保样本梯度的期望与真实梯度同向。
## 三、关键算法：REINFORCE（蒙特卡洛策略梯度）
- **核心思想**：利用蒙特卡洛采样估计梯度，通过一幕数据的回报作为目标值更新策略参数。
- **输入**：策略$π$、可导策略函数$π(a|s,θ)$、步长$α$。
- **流程**：
  1. 按策略π生成一幕数据：$S₀,A₀,R₁,...,S_T$；
  2. 对每一步t（0≤t<T），计算回报$G_t$；
  3. 更新参数：$θ ← θ + αγ^t G_t [∇π(A_t|S_t,θ)/π(A_t|S_t,θ)]$（$γ$为折扣因子）。
- **特点**：无偏估计（蒙特卡洛方法），但方差大（依赖单幕回报）。
## 四、总结
- **总结**：策略梯度方法扩展了强化学习的策略表示能力（尤其是连续动作和随机策略场景），策略梯度定理提供了理论保证，REINFORCE作为基础算法虽方差大，但为后续改进（如引入基线、行动器-评判器）奠定了基础。