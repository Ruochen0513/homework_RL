## <center>《强化学习导论》第四章读书报告
##### <center> 智能科学与技术 2213530 张禹豪
### 一、动态规划概述
&emsp;&emsp;动态规划（Dynamic Programming, DP）是解决马尔可夫决策过程（MDP）的经典方法，通过计算值函数（如状态值函数或动作值函数）来寻找最优策略。DP的核心思想是将贝尔曼方程转化为迭代更新规则。其优势在于理论完备性，但局限性在于需要环境模型完全已知且计算复杂度高，适用于有限状态空间的问题。
&emsp;&emsp;本章系统阐述了如何利用DP进行策略评估、策略改进，以及策略迭代与值迭代算法，为后续无模型方法奠定理论基础。
### 二、策略评估
&emsp;&emsp;策略评估是指计算给定策略的值函数。通过迭代更新值函数，使其逼近贝尔曼方程的解。具体来说，策略评估算法包括同步更新和异步更新两种方式，其中异步更新算法包括原始异步更新、随机异步更新和期望异步更新。异步更新算法的优势在于计算效率高，但收敛性不易保证。
- **目标**：计算给定策略$\pi$的值函数$v_{\pi}$。
- **方法**：迭代更新值函数，使其逼近贝尔曼方程的解。
- **算法**：用某种策略（比如随机选取动作），通过迭代法，计算该问题的各个状态的状态价值 $V^\pi(s)$。当然当前的这个策略未必是最优的，得到的状态值也未必理想，但它就是当前这个策略的评估结果。迭代公式是贝尔曼方程：$$
V^\pi(s)=E_aE_s[R(s,a,s')+\gamma V^\pi(s')$$&emsp;&emsp;得到的各个状态值可用一个表格来表示。
- **步骤**：
  - 初始化值函数 $V(s)$；
  - 通过贝尔曼方程迭代更新值函数 $V(s)$：
        $$V(s)\leftarrow E_aE_s[R(s,a,s')+\gamma V(s')]$$
  - 当值函数变化范围小于阈值$\theta$时终止。
- 支持“就地更新”（in-place update），利用最新值加速收敛。
- **问题**：在策略评估步骤中，如何通过求解贝尔曼方程来得到状态值$V_{\pi k}$呢？$$V_{\pi k}=r_{\pi k}+\gamma P_{\pi k}v_{\pi k}$$&emsp;&emsp;**显式解**：$$v_{\pi k}=(I-\gamma P_{\pi k})^{-1}r_{\pi k}$$&emsp;&emsp;**迭代解法**：$$v^{(j+1)}_{\pi k}=r_{\pi k}+\gamma P_{\pi k}v^{(j)}_{\pi k},j=0,1,2,...$$
### 三、策略改进与策略迭代
- **目标**：改进给定策略$\pi$。
- **贪心策略**：选择使得动作值函数$Q^\pi(s,a)$最大的动作$a$。
- **策略改进定理**：
![alt text](imgs/1.png)
- **策略迭代**：通过策略评估和策略改进交替进行，直到策略收敛。
- **算法**：
  - 初始化策略$\pi$；
  - 策略评估：计算当前策略$\pi$的值函数$V^\pi$；
  - 策略改进：根据值函数$V^\pi$更新策略$\pi$；
  - 当策略不再改变时终止。
- **步骤**：
&emsp;&emsp;对上面得到的价值表，通过迭代，求出各个状态s各个动作a的动作值函数值$Q^\pi(s,a)$。
&emsp;&emsp;迭代公式为：$$Q^\pi(s,a)=E_{s'}[R(s,a,s')+\gamma V^\pi (s')]$$&emsp;&emsp;然后用得到的改进策略。即对于每个状态s，选择使得$Q^\pi(s,a)$最大的动作a，作为新的策略。即：$$\pi'(s)=argmax_aQ^\pi(s,a)$$&emsp;&emsp;这样就得到了一个新的策略，然后再用这个新的策略进行策略评估，策略改进，如此循环，直到策略收敛。
&emsp;&emsp;初始策略的设置，可以采用随机策略或者某种启发式方法。随机策略是随机选择动作，目的是用各个动作与环境交互，探索环境，获取更全面的数据。启发式方法，是通过专家知识或经验指导，选择相对较优的策略，可以加快模型的收敛速度。
### 四、值迭代
&emsp;&emsp;对于上面的问题，不一定让策略评估和策略改进反复交替多次，而是用贝尔曼最优方程，一次性确定各个状态的$V^\pi(s)$，再用这些最优状态值函数$V^\pi(s)$计算动作值函数$Q(s,a)$，最后取$Q(s,a)$最大的动作，这就是值函数迭代算法。
&emsp;&emsp;**如何求解贝尔曼最优方程？**$$v=f(v)=max_\pi(r_\pi+\gamma P_\pi v)$$&emsp;&emsp;我们了解到压缩映射定理给出了一种迭代算法：$$v_{k+1}=f(v_k)=max_\pi (r_\pi+\gamma P_\pi v_k),k=1,2,3...$$&emsp;&emsp;其中$v_0$可以是任意值。
- 该算法最终可以找到最优状态值和最优策略。
- 这种算法被称为**值迭代**！
- **目标**：计算最优值函数$V^*(s)$。
- **方法**：迭代更新值函数，使其逼近贝尔曼最优性方程的解。
- **算法步骤**：
  - **步骤1**：策略更新。此步骤是求解$$\pi_{k+1}=argmax_\pi(r_\pi+\gamma P_\pi v_k)$$
  - **步骤2**：价值更新。此步骤是求解$$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}} v_k$$
### 五、异步动态规划
- **目标**：提高DP的计算效率。
- **核心思想**：放弃全局状态遍历，按任意顺序更新状态值，提升计算效率。
- **方法**：
  - ​原位更新：直接覆盖旧值，减少存储开销。
  - ​优先扫描​：优先更新值变化大的状态，加速收敛。
  - ​实时动态规划：仅更新与智能体交互相关的状态。
- **优势**：适用于大规模问题，提升计算效率。
### 六、动态规划的局限性
- ​**维度灾难**：状态空间随维度指数增长，计算复杂度剧增。
- ​**依赖环境模型**：需已知转移概率$p(s',r|s,a)$，实际场景中难以满足。
- **计算资源限制**：大规模问题需要近似方法（如函数逼近）配合。
### 七、总结与启示
&emsp;&emsp;动态规划是强化学习的理论基础，尽管实际应用中需结合近似方法（如函数逼近或蒙特卡洛采样），但其核心思想——通过值函数引导策略优化——贯穿整个领域。
- **理论基石**：DP首次系统展示了如何通过值函数迭代求解MDP，证明了策略改进定理等重要理论。
- **​算法影响**：策略迭代与值迭代的思想启发了Q-learning、策略梯度等无模型方法。
- **​实践意义**：尽管受限于完全已知环境，DP在机器人控制、游戏AI等模型已知场景仍有应用价值。
- **未来方向**：结合采样（如蒙特卡洛）与函数逼近（如深度学习），突破DP局限，推动强化学习在复杂场景的应用。